---
# try also 'default' to start simple
theme: default
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
# background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: Malayalam in the age of Artificial Intelligence
info: |
  ## Presentation for Symposium at Malayalam University

  Made with [Sli.dev](https://sli.dev)
# apply UnoCSS classes to the current slide
class: text-center
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true
# duration of the presentation
duration: 35min
fonts:
  sans: Roboto, Manjari
  serif: Roboto Slab
  mono: JetBrains Mono
---

# ‡¥®‡¥ø‡µº‡¥Æ‡¥ø‡¥§‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø ‡¥≠‡¥æ‡¥∑ ‡¥™‡¥†‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡µÜ‡¥ô‡µç‡¥ô‡¥®‡µÜ?
## How AI learns Malayalam?


<!--
introduction
-->
---
layout: default
---

# Language

### Phonetics, Phonology
Speech Sounds, Phonemes

### Morphology

Words and Forms

### Syntax

Sentences and Phrases

### Semantics

Literal meaning of various kinds

### Pragmatics

Language usage, Meaning in context of discourse

<style>
h3 {
  background-color: #2B90B6;
  background-image: linear-gradient(45deg, #4EC5D4 10%, #146b8c 20%);
  background-size: 100%;
  -webkit-background-clip: text;
  -moz-background-clip: text;
  -webkit-text-fill-color: transparent;
  -moz-text-fill-color: transparent;
}
</style>



---
layout: two-cols
---

### ‡¥Ö‡¥™‡¥∞‡¥æ‡¥π‡µç‡¥®‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥Ö‡¥®‡¥®‡µç‡¥§‡¥™‡¥•‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡µΩ ‡¥Ü‡¥ï‡¥æ‡¥∂‡¥®‡µÄ‡¥≤‡¥ø‡¥Æ‡¥Ø‡¥ø‡µΩ ‡¥Ö‡¥µ‡µª ‡¥®‡¥ü‡¥®‡µç‡¥®‡¥ï‡¥®‡µç‡¥®‡µÅ.
### ‡¥≠‡µÄ‡¥Æ‡¥®‡µÅ‡¥Ç ‡¥Ø‡µÅ‡¥ß‡¥ø‡¥∑‡µç‡¥†‡¥ø‡¥∞‡¥®‡µÅ‡¥Ç ‡¥¨‡µÄ‡¥°‡¥ø ‡¥µ‡¥≤‡¥ø‡¥ö‡µç‡¥ö‡µÅ.
### ‡¥∏‡µÄ‡¥§‡¥Ø‡µÅ‡¥ü‡µÜ ‡¥Æ‡¥æ‡¥±‡µÅ‡¥™‡¥ø‡¥≥‡µº‡¥®‡µç‡¥®‡µç ‡¥∞‡¥ï‡µç‡¥§‡¥Ç ‡¥ï‡µÅ‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µÅ ‡¥¶‡µÅ‡¥∞‡µç‡¥Ø‡µã‡¥ß‡¥®‡µª.
### ‡¥ó‡µÅ‡¥∞‡µÅ‡¥µ‡¥æ‡¥Ø‡µÇ‡¥∞‡¥™‡µç‡¥™‡¥®‡µç ‡¥ú‡¥≤‡¥¶‡µã‡¥∑‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥∞‡µÅ‡¥®‡µç‡¥®‡µÅ ‡¥Ö‡¥®‡µç‡¥®‡µç.
### ‡¥Ö‡¥Æ‡µç‡¥™‡¥≤‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥Ö‡¥ï‡¥æ‡µΩ‡¥µ‡¥ø‡¥≥‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡µæ ‡¥§‡µÜ‡¥≥‡¥ø‡¥Ø‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥®‡µç‡¥ß‡µç‡¥Ø‡¥Ø‡¥ø‡µΩ ‡¥Ö‡¥µ‡µæ ‡¥Ö‡¥µ‡¥®‡µã‡¥ü‡µç ‡¥ö‡µã‡¥¶‡¥ø‡¥ö‡µç‡¥ö‡µÅ,
### "‡¥á‡¥®‡¥ø‡¥Ø‡µÅ‡¥Ç ‡¥®‡µÄ ‡¥á‡¥§‡µÅ‡¥µ‡¥¥‡¥ø ‡¥µ‡¥∞‡¥ø‡¥≤‡µç‡¥≤‡µá, ‡¥Ü‡¥®‡¥ï‡¥≥‡µÜ‡¥Ø‡µÅ‡¥Ç ‡¥§‡µÜ‡¥≥‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µç?"

::right::

<Youtube id="q8uvDnthq94?start=383" width="500" height="300"/>

<!--
https://www.youtube.com/clip/UgkxfJYLEyYeRCq3pkBOfLimnfqnjY2j3-Ra
-->

---
layout: center
---

### ‡¥≠‡¥æ‡¥ó‡¥Ç 1

# ‡¥≠‡¥æ‡¥∑ ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥™‡¥†‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥Ç?

---
layout: image-right
image: 960px-Noam_Chomsky_portrait_2017_retouched.webp
---
# Noam Chomsky

---
layout: iframe-right
url: Chomsky1957.pdf
---

# Syntactic structures (1957)
<div class="color-blue">
"Colorless green ideas sleep furiously"
</div>
An example of a grammatically correct sentence that has no discernible meaning

Independence of
- syntax (the study of sentence structures)
- semantics (the study of meaning)

---
layout: image-right
image: chomsky-tree.webp
backgroundSize: 70%
---

# Noam Chomsky

## Syntactic structures (1957)

<v-clicks class="color-blue">

* grammar is autonomous and independent of meaning.
* probabilistic models give no particular insight into some of the basic problems of syntactic structure.
</v-clicks>

---
layout: two-cols
---

# Chomsky's Theory

* Humans have innate "Universal Grammar" - built-in language rules we're born with
* Language cannot be learned from examples alone due to "poverty of the stimulus"
* Children learn language from limited input (few million words by age 5)
* Language requires understanding deep structural rules, not just pattern matching

::right::

# Large Language Models

* Learn language entirely from statistical patterns in data - no built-in grammar rules
* Require massive training data (billions to trillions of words)
* Generate grammatically correct and coherent text through pattern prediction alone
* Challenge: May lack true understanding despite producing fluent language-machinery


---
layout: fact
---


## LLMs demonstrate that powerful statistical learning from data can produce sophisticated language without innate grammar

<v-click class="color-blue">
but use far more data than humans and may lack deeper comprehension.
</v-click>
---
layout: iframe-left
url: 2021-bender-parrots.pdf
---

# Stochastic Parrots ü¶ú
Emily Bender and others - 2021

<v-clicks class="color-blue">

* **No true understanding**: <small>LLMs are systems for stitching together linguistic forms from vast training data, without any reference to context or meaning - it's the human makes sense of it, not the computer</small>
* **Form without meaning**: <small>For LLMs, words may correspond only to other words/patterns fed into their training data, whereas in human minds, words and language correspond to things one has experienced</small>
* **Evidence from failures**: <small>The tendency of LLMs to pass off false information as fact (hallucinations) is held as support that they can't connect words to a comprehension of the world, as humans do</small>
</v-clicks>

---
layout: image
image: sam-altman-tweet.webp
backgroundSize: 50%
---

---
layout: statement
---

# Language != Intelligence

---
layout: default
---

# Hallucination
*   [OpenAI: Why language models hallucinate](https://openai.com/index/why-language-models-hallucinate/)
*   [Oxford University: Large Language Models pose risk to science with false answers, says Oxford study](https://www.ox.ac.uk/news/2023-11-20-large-language-models-pose-risk-science-false-answers-says-oxford-study)
*   [New York Times: A.I. Is Getting More Powerful, but Its Hallucinations Are Getting Worse](https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html) ([Archived Version](https://archive.is/CD7Ge))
*   [MIT Media Lab: People Overtrust AI-Generated Medical Advice despite Low Accuracy](https://www.media.mit.edu/publications/NEJM-AI-people-overtrust-ai-generated-medical-advice-despite-low-accuracy/)
*   [Business Insider: Why AI chatbots hallucinate, according to OpenAI researchers](https://www.businessinsider.com/why-ai-chatbots-hallucinate-openai-chatgpt-anthropic-claude-2025-9)
*   [Reuters: AI 'hallucinations' in court papers spell trouble for lawyers](https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/)
*   [Nature: AI chatbots are sycophants ‚Äî researchers say it‚Äôs harming science](https://www.nature.com/articles/d41586-025-03390-0)
*   [CNN: Parents of 16-year-old sue OpenAI, claiming ChatGPT advised on his suicide](https://www.cnn.com/2025/08/26/tech/openai-chatgpt-teen-suicide-lawsuit)
*   [Financial Times: The ‚Äòhallucinations‚Äô that haunt AI: why chatbots struggle to tell the truth](https://www.ft.com/content/7a4e7eae-f004-486a-987f-4a2e4dbd34fb) ([Archived Version](https://archive.is/P1Wpc))
*   [The Guardian: ‚ÄòSycophantic‚Äô AI chatbots tell users what they want to hear, study shows](https://www.theguardian.com/technology/2025/oct/24/sycophantic-ai-chatbots-tell-users-what-they-want-to-hear-study-shows)

---
layout: iframe
url: 2510.12766.pdf
---

<!-- Language Models Model Language -->



---
layout: image
image: ex-gen-gram.webp
backgroundSize: 75%
---
<div class="absolute items-center bottom-30px">
Example of a simple context-free generative grammar and derived sentence, based on a corpus
</div>

<!-- On the Compatibility of Generative AI and Generative Linguistics
https://arxiv.org/html/2411.10533v2
-->

---
layout: image
image: CS-hierarchy.webp
backgroundSize: 75%
---

<div class="absolute items-center bottom-30px">
Chomsky-Sch√ºtzenberger hierarchy
</div>

<!-- On the Compatibility of Generative AI and Generative Linguistics
https://arxiv.org/html/2411.10533v2
-->

---
layout: iframe
url: santhosh-mlmorph-W19-6801.pdf
---


---
layout : iframe
url: https://morph.smc.org.in/
---

---
layout: image
image: mlmorph-analysis.webp
backgroundSize: 80%
---

---
layout: image
image: mlmorph-generator.webp
backgroundSize: 80%
---

---
layout: image
image: mlmoprh-spellchecker.webp
backgroundSize: 80%
---

---
layout: image-right
image: sutton.webp
---

# Richard Sutton

Winner of 2024 Turing Award ("Nobel Prize of Computing,") for Reinforcement Learning

<!--
Richard Sutton is a Canadian computer scientist known for his pioneering work in reinforcement learning, a branch of AI that involves agents learning from interaction and consequences. He is a professor at the University of Alberta and was awarded the 2024 Turing Award for his foundational contributions to the field. Sutton developed influential algorithms like temporal-difference learning and policy-gradient methods, which are used in machine learning and as models for natural learning in psychology and neuroscience
-->
---
layout: iframe-left
url: http://www.incompleteideas.net/IncIdeas/BitterLesson.html
---
<q  class="color-orange">
General methods that leverage computational power will outperform more complex systems that integrate domain-specific human knowledge
</q>

because they take better advantage of Moore's law

<v-clicks class="color-blue">

* Stop trying to understand how intelligence works
* Stop encoding domain knowledge (whether innate grammar or grounded meaning)
* Just scale computation and learning
</v-clicks>

<!--
Core argument: AI researchers have often tried to build knowledge into their agents, which helps in the short term and is personally satisfying to the researcher, but in the long run it plateaus and even inhibits further progress, and breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning

-->

---
layout: default
---

# ‡¥ï‡¥• ‡¥á‡¥§‡µÅ‡¥µ‡¥∞‡µÜ

<v-clicks class="color-blue">

* Chomsky: Build in innate linguistic structure
* Bender: LLMs lack grounded meaning and understanding
* Sutton: None of that matters - just add more compute
</v-clicks>

---
layout: two-cols
---

# Sutton - October 2025

* LLMs are dead end
* Need world model
* Text is not enough

::right::
<Youtube width=500 height=300 id="21EYKqUsPfg"/>


---
layout: center
---

### ‡¥≠‡¥æ‡¥ó‡¥Ç 2

# ‡¥é‡¥¥‡µÅ‡¥§‡¥ø‡¥Ø ‡¥≠‡¥æ‡¥∑‡¥Ø‡¥ø‡µΩ ‡¥®‡¥ø‡¥®‡µç‡¥®‡µç ‡¥é‡¥®‡µç‡¥§‡µÅ ‡¥™‡¥†‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥Ç?

---
layout: image-right
image: George_Kingsley_Zipf_1917.webp
---
# George Kingsley Zipf
1902 ‚Äì 1950

<div v-click class="mt-15 text-xl color-blue">
Frequency of occurrence of words Inversely proportional to the rank in this frequency of occurrence.
</div>


<div v-click class="mt-15 text-xl color-blue">
‡¥í‡¥∞‡µÅ ‡¥≠‡¥æ‡¥∑‡¥Ø‡¥ø‡¥≤‡µÜ ‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ü‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥µ‡¥∞‡µã‡¥π‡¥£‡¥ï‡µç‡¥∞‡¥Æ‡¥§‡µç‡¥§‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥™‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥Ø‡¥ø‡µΩ,
‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Ç,
‡¥Ö‡¥§‡¥ø‡¥®‡µç‡¥±‡µá ‡¥Ü‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥§‡µç‡¥§‡¥ø‡¥®‡µç ‡¥µ‡¥ø‡¥™‡¥∞‡µÄ‡¥§‡¥æ‡¥®‡µÅ‡¥™‡¥§‡¥§‡µç‡¥§‡¥ø‡¥≤‡¥æ‡¥Ø‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç
</div>

---
layout: two-cols
---
# Zipf's Law

<div  class="mt-15 text-xl color-blue">
 ‡¥è‡¥±‡µç‡¥±‡¥µ‡µÅ‡¥Ç ‡¥ï‡µÇ‡¥ü‡µÅ‡¥§‡µΩ ‡¥§‡¥µ‡¥£ ‡¥µ‡¥∞‡µÅ‡¥®‡µç‡¥® ‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µç ‡¥§‡µä‡¥ü‡µç‡¥ü‡µÅ‡¥™‡µÅ‡¥±‡¥ï‡¥ø‡µΩ ‡¥µ‡¥∞‡µÅ‡¥®‡µç‡¥® ‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥≥‡µÜ‡¥ï‡µç‡¥ï‡¥æ‡µæ ‡¥á‡¥∞‡¥ü‡µç‡¥ü‡¥ø ‡¥µ‡¥∞‡µÅ‡¥Ç
</div>

::right::

## English

| Word | Percentag| Ratio |
|-------|-----------|--------|
|the    | 7%| 1
|of   |    3.5%   | (7 * ¬Ω)|
|and |   2.3%    |(7 * ‚Öì)}
|to |     1.75%  |(7* ¬º)|

<div v-click>

‡¥á‡¥Ç‡¥ó‡µç‡¥≤‡µÄ‡¥∑‡µç ‡¥≠‡¥æ‡¥∑‡¥Ø‡¥ø‡¥≤‡µÜ 25% ‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç "the," "be," "to," "of," "and," "a," "in," "that," "have," and "I." ‡¥é‡¥®‡µç‡¥®‡µÄ ‡¥™‡¥§‡µç‡¥§‡µÅ‡¥µ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µä‡¥®‡µç‡¥®‡¥æ‡¥£‡µç.

</div>

---
layout: image
image: 1365px-Zipf_30wiki_en_labels.webp
backgroundSize: 70%
---

<!--
Zipf's law plot for the first 10 million words in 30 Wikipedias (as of October 2015)
-->
---
layout: image
image: ml-zipfs-law.webp
backgroundSize: 75%
---

## ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç

<div class="absolute items-center bottom-30px">
From 205k unique sentences from SMC corpus. Prepared by Kavya Manohar
</div>

---
layout: iframe
url: https://kavyamanohar.com/post/malayalam-entropy
---

---
layout: center
class: text-center
---

# ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥§‡µç‡¥§‡¥ø‡µΩ

<div class="text-xl mt-5">
‡¥è‡¥±‡µç‡¥±‡¥µ‡µÅ‡¥Ç ‡¥ï‡µÇ‡¥ü‡µÅ‡¥§‡µΩ ‡¥Ü‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥® ‡¥Ö‡¥ï‡µç‡¥∑‡¥∞‡¥Æ‡µá‡¥§‡µç?
</div>

<div class="text-xl mt-5" v-click>
‡¥è‡¥±‡µç‡¥±‡¥µ‡µÅ‡¥Ç ‡¥ï‡µÅ‡¥±‡¥µ‡µç ‡¥Ü‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥® ‡¥Ö‡¥ï‡µç‡¥∑‡¥∞‡¥Æ‡µá‡¥§‡µç?
</div>


---
layout: image-right
image: 788px-AAMarkov.webp
---
# Andrey Markov
4 June 1856 ‚Äì 20 July 1922

<div class="text-xl mt-6 color-blue">
What happens next depends only on the state of affairs now.
</div>

---
layout: image
image: 960px-Chess_game_Staunton_No._6.webp
backgroundSize: contain
---

---
layout: image
image: markov-good-morning.webp
backgroundSize: contain
---
<!-- source  https://www.sitepen.com/blog/exploring-the-creative-possibilities-of-markov-chains-for-text-generation -->

---
layout: image
image: markov-good-morning-2.webp
backgroundSize: contain
---

<!-- source  https://www.sitepen.com/blog/exploring-the-creative-possibilities-of-markov-chains-for-text-generation -->

---
layout: center
---

<div class="text-xl mt-10" >
    ‡¥é‡¥®‡µç‡¥§‡µç ‡¥™‡µç‡¥∞‡¥π‡¥∏‡¥®‡¥æ‡¥£‡µç ‚Äå______
</div>


<div  class="text-xl mt-10" v-click>
‡¥Ö‡¥µ‡¥ø‡¥ü‡µÜ ‡¥ï‡¥≤‡µç‡¥Ø‡¥æ‡¥£‡¥Ç. ‡¥á‡¥µ‡¥ø‡¥ü‡µÜ ‚Äî---

</div>

<div class="text-xl mt-10" v-click>
‡¥ï‡µº‡¥£‡µª, ‡¥®‡µÜ‡¥™‡µç‡¥™‡µã‡¥≥‡¥ø‡¥Ø‡µª, ‚Äî--

</div>


---
layout: center
---

## ‡¥Ö‡¥™‡¥∞‡¥æ‡¥π‡µç‡¥®‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥Ö‡¥®‡¥®‡µç‡¥§‡¥™‡¥•‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡µΩ ‡¥Ü‡¥ï‡¥æ‡¥∂‡¥®‡µÄ‡¥≤‡¥ø‡¥Æ‡¥Ø‡¥ø‡µΩ ‡¥Ö‡¥µ‡µª ‡¥®‡¥ü‡¥®‡µç‡¥®‡¥ï‡¥®‡µç‡¥®‡µÅ.
## ‡¥≠‡µÄ‡¥Æ‡¥®‡µÅ‡¥Ç ‡¥Ø‡µÅ‡¥ß‡¥ø‡¥∑‡µç‡¥†‡¥ø‡¥∞‡¥®‡µÅ‡¥Ç ‡¥¨‡µÄ‡¥°‡¥ø ‡¥µ‡¥≤‡¥ø‡¥ö‡µç‡¥ö‡µÅ.
## ‡¥∏‡µÄ‡¥§‡¥Ø‡µÅ‡¥ü‡µÜ ‡¥Æ‡¥æ‡¥±‡µÅ‡¥™‡¥ø‡¥≥‡µº‡¥®‡µç‡¥®‡µç ‡¥∞‡¥ï‡µç‡¥§‡¥Ç ‡¥ï‡µÅ‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡µÅ ‡¥¶‡µÅ‡¥∞‡µç‡¥Ø‡µã‡¥ß‡¥®‡µª.
## ‡¥ó‡µÅ‡¥∞‡µÅ‡¥µ‡¥æ‡¥Ø‡µÇ‡¥∞‡¥™‡µç‡¥™‡¥®‡µç ‡¥ú‡¥≤‡¥¶‡µã‡¥∑‡¥Æ‡¥æ‡¥Ø‡¥ø‡¥∞‡µÅ‡¥®‡µç‡¥®‡µÅ ‡¥Ö‡¥®‡µç‡¥®‡µç.
## ‡¥Ö‡¥Æ‡µç‡¥™‡¥≤‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥Ö‡¥ï‡¥æ‡µΩ‡¥µ‡¥ø‡¥≥‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡µæ ‡¥§‡µÜ‡¥≥‡¥ø‡¥Ø‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥®‡µç‡¥ß‡µç‡¥Ø‡¥Ø‡¥ø‡µΩ ‡¥Ö‡¥µ‡µæ ‡¥Ö‡¥µ‡¥®‡µã‡¥ü‡µç ‡¥ö‡µã‡¥¶‡¥ø‡¥ö‡µç‡¥ö‡µÅ,
## "‡¥á‡¥®‡¥ø‡¥Ø‡µÅ‡¥Ç ‡¥®‡µÄ ‡¥á‡¥§‡µÅ‡¥µ‡¥¥‡¥ø ‡¥µ‡¥∞‡¥ø‡¥≤‡µç‡¥≤‡µá, ‡¥Ü‡¥®‡¥ï‡¥≥‡µÜ‡¥Ø‡µÅ‡¥Ç ‡¥§‡µÜ‡¥≥‡¥ø‡¥ö‡µç‡¥ö‡µÅ‡¥ï‡µä‡¥£‡µç‡¥ü‡µç?"


<!--
https://www.youtube.com/clip/UgkxfJYLEyYeRCq3pkBOfLimnfqnjY2j3-Ra
-->
---
layout: image-right
image: Zellig_Harris_(1909‚Äì1992).webp
---
# Zellig Harris
1909 ‚Äì 1992

<div class="text-xl mt-6 color-blue">
‚Äúwords that occur in similar contexts tend to have similar meanings‚Äù
</div>

<div class="text-xl mt-6 color-blue">
How the statistical patterns of human word usage can be used to figure out
what people mean,
at least to a level sufficient for information access

</div>
<div class="text-xl mt-6 color-blue">
- Distributional hypothesis - Harris, 1954

</div>

---
layout: default
url: https://ig.ft.com/generative-ai/
---
<!-- https://ig.ft.com/generative-ai/ -->

---
layout: image
image: 1448px-Distributional_semantics.webp
backgroundSize: 60%
---

---
layout: image
image: 3dplot-500x381.webp
backgroundSize: 60%
---
<!--
https://corpling.hypotheses.org/495
-->

---
layout: image-left
image: Blog_image1-300x191.webp
background-size: contain
---
## Man + royal = King
## Woman + royal = Queen

Similarly

### Man + medical occupation = Doctor
### Woman + medical occupation = Nurse
<!-- http://blogs.ischool.berkeley.edu/w231/2021/05/31/machine-learning-bias-in-word-embedding-algorithms/ -->


---
layout: center
---
<h2>

‡¥ï‡¥û‡µç‡¥û‡¥ø +  ‡¥™‡¥û‡µç‡¥ö‡¥∏‡¥æ‡¥∞ = ‡¥™‡¥æ‡¥Ø‡¥∏‡¥Ç

</h2>

<h2 v-click>
 ‡¥ö‡¥æ‡¥Ø - ‡¥™‡¥æ‡µΩ = ‡¥ï‡¥ü‡µç‡¥ü‡µª ‡¥ö‡¥æ‡¥Ø

</h2>

<h2 v-click>
 ‡¥™‡¥æ‡µΩ + ‡¥ï‡¥û‡µç‡¥û‡¥ø = ‡¥™‡¥æ‡µΩ‡¥ï‡µç‡¥ï‡¥û‡µç‡¥û‡¥ø

</h2>

<h2 v-click>
 ‡¥™‡¥æ‡¥Ø‡¥∏‡¥Ç - ‡¥™‡¥û‡µç‡¥ö‡¥∏‡¥æ‡¥∞ = ‡¥ï‡¥û‡µç‡¥û‡¥ø

</h2>

---
layout: center
---

<h2>
        ‡¥ï‡µá‡¥∞‡¥≥‡¥Ç +  ‡¥§‡¥≤‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Ç =  ‡¥§‡¥ø‡¥∞‡µÅ‡¥µ‡¥®‡¥®‡µç‡¥§‡¥™‡µÅ‡¥∞‡¥Ç
</h2>

<h2 v-click>
 ‡¥ï‡µº‡¥£‡¥æ‡¥ü‡¥ï + ( ‡¥ï‡µá‡¥∞‡¥≥‡¥Ç - ‡¥§‡¥ø‡¥∞‡µÅ‡¥µ‡¥®‡¥®‡µç‡¥§‡¥™‡µÅ‡¥∞‡¥Ç) = ?

</h2>
---
layout: image
image: kanji.webp
---

---
layout: image
image: attention-paper.webp
backgroundSize: 50%
---


---
layout: image
image: transformer_self-attention_visualization_3.webp
backgroundSize: 50%
---

---
layout: iframe
url: language_models_are_unsupervised_multitask_learners.pdf
---


---
layout: iframe
url: 2005.14165.pdf
---

---
layout: image
image: english-tokens.webp
backgroundSize: 70%
---
<!-- Source https://huggingface.co/spaces/santhosh/tokenizers-languages -->

---
layout: image
image: malayalam-tokens.webp
backgroundSize: 70%
---



<!-- Source https://huggingface.co/spaces/santhosh/tokenizers-languages -->
---
layout: image
image: top-langs-token-length.webp
---

<!-- Source https://huggingface.co/spaces/santhosh/tokenizers-languages -->

---
layout: image
image: medium-token-length.webp
backgroundSize: 70%
---

---
layout: image
image: karpathy-tokenization.webp
backgroundSize: 66%
---


---
layout: iframe
url: https://kavyamanohar.com/post/malayalam-morphological-complexity/
---


---
layout: image
image: tsd1030a.webp
backgroundSize: 50%
---
---
layout: image
image: TypeTokengrowth-comparison.webp
backgroundSize: 60%
---
---
layout: two-cols
---

# English Word Order

## Subject-Verb-Object

* The boy is reading a book

::right::

# ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç - Free Word Order

* ‡¥∞‡¥æ‡¥Æ‡µª ‡¥∞‡¥æ‡¥µ‡¥£‡¥®‡µÜ ‡¥ï‡µä‡¥®‡µç‡¥®‡µÅ

* ‡¥∞‡¥æ‡¥µ‡¥£‡¥®‡µÜ ‡¥∞‡¥æ‡¥Æ‡µª ‡¥ï‡µä‡¥®‡µç‡¥®‡µÅ

* ‡¥ï‡µä‡¥®‡µç‡¥®‡µÅ ‡¥∞‡¥æ‡¥µ‡¥£‡¥®‡µÜ ‡¥∞‡¥æ‡¥Æ‡µª

* ‡¥ï‡µä‡¥®‡µç‡¥®‡µÅ ‡¥∞‡¥æ‡¥Æ‡µª ‡¥∞‡¥æ‡¥µ‡¥£‡¥®‡µÜ

---
layout: center
---

# Training data

## 93% of ChatGPT-3‚Äôs data set was in English

### 0.00165% was in Malayalam

<!--
https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv
-->

---
layout: center
---

#  ‡¥á‡¥®‡µç‡¥±‡µº‡¥®‡µÜ‡¥±‡µç‡¥±‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡¥â‡¥≥‡µç‡¥≥‡¥ü‡¥ï‡µç‡¥ï‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡µç‡¥µ‡¥≠‡¥æ‡¥µ‡¥Ç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?

---
layout: center
---

# ‡¥®‡¥®‡µç‡¥¶‡¥ø
